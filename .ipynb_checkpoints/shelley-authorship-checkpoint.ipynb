{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def clean_text(text):\n",
    "    #lowercase all\n",
    "    ocr_lower = text.lower()\n",
    "    #tokenize, remove punctuation and numbers, remove tabs, newlines, etc.\n",
    "    ocr_cleaner = ocr_lower.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    try:\n",
    "        ocr_cleaner = unicode(ocr_cleaner)\n",
    "    except:\n",
    "        ocr_cleaner = ocr_cleaner.decode('latin-1')\n",
    "    doc = nlp(ocr_cleaner)\n",
    "    ocr_tokens = []\n",
    "    for token in doc:\n",
    "        ocr_tokens.append(unicode(token))\n",
    "        \n",
    "    no_numbers_or_punct = []\n",
    "    for token in ocr_tokens:\n",
    "        if token.isalpha():\n",
    "            no_numbers_or_punct.append(token)\n",
    "        else:\n",
    "\n",
    "            new_token = \"\"\n",
    "            for letter in token:\n",
    "                if letter.isalpha():\n",
    "                    new_token += letter\n",
    "            if new_token != \"\":\n",
    "                no_numbers_or_punct.append(new_token)  \n",
    "        \n",
    "    return no_numbers_or_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "percy = ['corpus/st-irvyne.txt', 'corpus/on-love-and-other-essays.txt', 'corpus/zastrozzi.txt']\n",
    "mary = ['corpus/valperga.txt', 'corpus/history-six-weeks-tour.txt', 'corpus/last-man.txt']\n",
    "franken_versions = ['corpus/1823_plain.txt','corpus/1831_plain.txt', 'corpus/1818_plain.txt']\n",
    "\n",
    "#process \n",
    "metadata = []\n",
    "texts = []\n",
    "#load texts\n",
    "import glob\n",
    "files = glob.glob(\"corpus/*.txt\")\n",
    "for i in files:\n",
    "    with open(i) as f:\n",
    "        txt = f.read()\n",
    "        #tokenize\n",
    "        processed = clean_text(txt)\n",
    "        #get word counts\n",
    "        wc = len(processed)\n",
    "    texts.append(processed)\n",
    "    filename = i.replace('corpus/', '').replace('.txt', '')\n",
    "    if i in percy:\n",
    "        row = [\"percy\", filename, wc] \n",
    "    if i in mary: \n",
    "        row = [\"mary\", filename, wc]\n",
    "    if i in franken_versions:\n",
    "        row = [\"franken\", filename, wc]\n",
    "    metadata.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>filename</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>franken</td>\n",
       "      <td>1823_plain</td>\n",
       "      <td>72594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mary</td>\n",
       "      <td>valperga</td>\n",
       "      <td>157121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>percy</td>\n",
       "      <td>st-irvyne</td>\n",
       "      <td>31817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>franken</td>\n",
       "      <td>1831_plain</td>\n",
       "      <td>75760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>franken</td>\n",
       "      <td>1818_plain</td>\n",
       "      <td>72621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mary</td>\n",
       "      <td>history-six-weeks-tour</td>\n",
       "      <td>18968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mary</td>\n",
       "      <td>last-man</td>\n",
       "      <td>176938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>percy</td>\n",
       "      <td>zastrozzi</td>\n",
       "      <td>30934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>percy</td>\n",
       "      <td>on-love-and-other-essays</td>\n",
       "      <td>28600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                  filename  word_count\n",
       "0  franken                1823_plain       72594\n",
       "1     mary                  valperga      157121\n",
       "2    percy                 st-irvyne       31817\n",
       "3  franken                1831_plain       75760\n",
       "4  franken                1818_plain       72621\n",
       "5     mary    history-six-weeks-tour       18968\n",
       "6     mary                  last-man      176938\n",
       "7    percy                 zastrozzi       30934\n",
       "8    percy  on-love-and-other-essays       28600"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store rows as pickle files\n",
    "import pickle\n",
    "import pandas as pd\n",
    "with open('texts.pickle', 'wb') as handle:\n",
    "    pickle.dump(texts, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('metadata.pickle', 'wb') as handle:\n",
    "    pickle.dump(metadata, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#make df\n",
    "df = pd.DataFrame.from_records(metadata, columns=[\"author\", \"filename\", \"word_count\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 9)\n",
      "(1, 9)\n",
      "(2, 8)\n",
      "(3, 9)\n",
      "(4, 9)\n",
      "(5, 4)\n",
      "(6, 9)\n",
      "(7, 7)\n",
      "(8, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 22), (1, 22)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from collections import Counter\n",
    "\n",
    "def chunker(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "#separate training and test\n",
    "labels = []\n",
    "chunk_list = []\n",
    "counter_chunk_list = []\n",
    "test_chunk_list = []\n",
    "test_chunk_counter_list =[]\n",
    "for index, text in enumerate(texts):\n",
    "    chunks = chunker(text, 4000)\n",
    "    chunks = [u for u in chunks if len(u) > 3500]\n",
    "    shuffle(chunks)\n",
    "    chunks = chunks[:9]\n",
    "    counter_chunks = [Counter(g) for g in chunks]\n",
    "    if index in [0, 3, 4]:\n",
    "        test_chunk_list.extend(chunks)\n",
    "        test_chunk_counter_list.extend(counter_chunks)\n",
    "    if index in [1, 5, 6]:\n",
    "        chunk_list.extend(chunks)\n",
    "        counter_chunk_list.extend(counter_chunks)\n",
    "        for z in chunks:\n",
    "            labels.append(0)\n",
    "    if index in [2, 7, 8]:\n",
    "        chunk_list.extend(chunks)\n",
    "        counter_chunk_list.extend(counter_chunks)\n",
    "        for z in chunks:\n",
    "            labels.append(1)\n",
    "    print(index, len(chunks))\n",
    "Counter(labels).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import requests\n",
    "\n",
    "    words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words')\n",
    "    stoplist1 = words.text.split(\"\\r\\n\")\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stoplist2 = set(stopwords.words('english'))\n",
    "\n",
    "    stoplist1.extend(stoplist2)\n",
    "\n",
    "    fullstops = list(set(stoplist1))\n",
    "    with open('fullstops.pickle', 'wb') as handle2:\n",
    "        pickle.dump(fullstops, handle2, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('fullstops.pickle', 'rb') as handle5:\n",
    "    fullstops = pickle.load(handle5)\n",
    "len(test_chunk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train chunks to dictionaries\n",
    "stop_features_train_list = dictionaries_of_features(counter_chunk_list, fullstops)\n",
    "stop_features_train_list[0]['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_features_test_list = dictionaries_of_features(test_chunk_counter_list, fullstops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples = stop_features_train_list + stop_features_test_list\n",
    "len(stop_features_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from application.selective_features import dictionaries_without_features, dictionaries_of_features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#train a model on train chunks\n",
    "#instantiate vectorizer\n",
    "v = DictVectorizer()\n",
    "#transform all\n",
    "X = v.fit_transform(all_samples)\n",
    "#convert to nonsparse\n",
    "scaled_vsm = X.toarray()\n",
    "\n",
    "#train logistic on first 44\n",
    "lr = LogisticRegression()\n",
    "lr.fit(scaled_vsm[0:44], labels)\n",
    "len(scaled_vsm[0:44]) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(scaled_vsm[44:])\n",
    "probs = lr.predict_proba(scaled_vsm[44:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "0 0.9999999985912944 1.408705616529943e-09\n",
      "0 0.9999999556899809 4.431001911505475e-08\n",
      "0 0.9999999971363603 2.8636396908267516e-09\n",
      "0 0.9999999143287365 8.567126355854404e-08\n",
      "0 0.9967294184566055 0.003270581543394548\n",
      "0 0.9999928107608674 7.189239132566104e-06\n",
      "0 0.9999999782250559 2.1774944096427544e-08\n",
      "0 0.9999999956929638 4.30703615015119e-09\n",
      "0 0.9998000605279981 0.0001999394720019693\n",
      "0 0.9999999833994171 1.660058296347535e-08\n",
      "0 0.999999875666924 1.2433307596990115e-07\n",
      "0 0.9940768118024936 0.005923188197506338\n",
      "0 0.9999999992555831 7.444168247128601e-10\n",
      "0 0.9999998529026518 1.470973482154441e-07\n",
      "0 0.9999994922789138 5.077210861349699e-07\n",
      "0 0.9999919707355938 8.029264406212783e-06\n",
      "0 0.999999994676023 5.323977003002001e-09\n",
      "0 0.9999999955350581 4.464941852639941e-09\n",
      "0 0.9999999951525894 4.8474105806390094e-09\n",
      "0 0.9999999678316999 3.216830017470894e-08\n",
      "0 0.9999988316708783 1.1683291216925451e-06\n",
      "0 0.9999980820169146 1.9179830853865363e-06\n",
      "0 0.999999981708437 1.8291563075054573e-08\n",
      "0 0.9998140122961935 0.0001859877038065106\n",
      "0 0.9999928851792885 7.114820711519009e-06\n",
      "0 0.9999999564155224 4.358447754625653e-08\n",
      "0 0.9999999991642912 8.357088198080614e-10\n"
     ]
    }
   ],
   "source": [
    "print(len(preds))\n",
    "for h,i in enumerate(preds):\n",
    "    print i, probs[h][0], probs[h][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "#train nb on first 44\n",
    "gnb.fit(scaled_vsm[0:44], labels)\n",
    "len(scaled_vsm[0:44]) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.99999999859', '0.00000000141')\n",
      "(0, '0.99999995569', '0.00000004431')\n",
      "(0, '0.99999999714', '0.00000000286')\n",
      "(0, '0.99999991433', '0.00000008567')\n",
      "(0, '0.99672941846', '0.00327058154')\n",
      "(0, '0.99999281076', '0.00000718924')\n",
      "(0, '0.99999997823', '0.00000002177')\n",
      "(0, '0.99999999569', '0.00000000431')\n",
      "(0, '0.99980006053', '0.00019993947')\n",
      "(0, '0.99999998340', '0.00000001660')\n",
      "(0, '0.99999987567', '0.00000012433')\n",
      "(0, '0.99407681180', '0.00592318820')\n",
      "(0, '0.99999999926', '0.00000000074')\n",
      "(0, '0.99999985290', '0.00000014710')\n",
      "(0, '0.99999949228', '0.00000050772')\n",
      "(0, '0.99999197074', '0.00000802926')\n",
      "(0, '0.99999999468', '0.00000000532')\n",
      "(0, '0.99999999554', '0.00000000446')\n",
      "(0, '0.99999999515', '0.00000000485')\n",
      "(0, '0.99999996783', '0.00000003217')\n",
      "(0, '0.99999883167', '0.00000116833')\n",
      "(0, '0.99999808202', '0.00000191798')\n",
      "(0, '0.99999998171', '0.00000001829')\n",
      "(0, '0.99981401230', '0.00018598770')\n",
      "(0, '0.99999288518', '0.00000711482')\n",
      "(0, '0.99999995642', '0.00000004358')\n",
      "(0, '0.99999999916', '0.00000000084')\n"
     ]
    }
   ],
   "source": [
    "preds = lr.predict(scaled_vsm[44:])\n",
    "probs = lr.predict_proba(scaled_vsm[44:])\n",
    "for h,i in enumerate(preds):\n",
    "    print (i, format(probs[h][0], '.11f'), (format(probs[h][1], '.11f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
